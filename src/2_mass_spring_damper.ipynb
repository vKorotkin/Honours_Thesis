{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of the 1D problem solutions to the mass spring damper IVP. \n",
    "\n",
    "\n",
    "The difference with the diffusion problem of the first notebook, is that the boundary conditions are not dirichlet boundary conditions. Rather, it is an IVP so the initial value and derivative is prescribed.\n",
    "\n",
    "The ansatz is still \n",
    "$$u=G(x)+D(x)*y^L$$\n",
    "but to satisfy the IVP constraint, $G(0), G'(0)=y(0), y'(0)$ and $D, \\frac{dD}{dx}=0$\n",
    "\n",
    "\n",
    "This still follows Berg & Nystrom's approach, but their examples only covered Dirichlet boundary conditions. The Neumann BC's are the new thing here. \n",
    "\n",
    "\n",
    "Convergence issues seem to happen when the maximum time covered comes close to the period of motion. The residual does not decrease anymore after a few iterations, and is not satisfactory. It is unclear why this happens. Possible reasons: optimization hitting local minimum, or issues relating to periodicity (i.e. the ansatz is not capable of dealing with that). \n",
    "\n",
    "\n",
    "Todo: Improve the optimizer used to e.g. BFGS line search. Investigate layer size/depth effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import init_printing\n",
    "init_printing(use_latex=True) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad, jacobian\n",
    "from autograd.misc.optimizers import adam\n",
    "from functools import reduce\n",
    "\n",
    "from reusable_functions import init_random_params, neural_net_predict, modded_basinhopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem Setup: \n",
    "#Make sure the residual and forcing functions correspond to your ODE.\n",
    "\n",
    "forcing_fn = lambda x: 0.\n",
    "layer_sizes=[1,10,10,1]\n",
    "nx=100\n",
    "u_0=1.; #initial position\n",
    "v_0=0.; #initial velocity\n",
    "G=lambda x:u_0+v_0*x #to satisfy BCs\n",
    "D=lambda x:x**2 #distance function\n",
    "\n",
    "x_space = np.linspace(0, 6, nx)\n",
    "def u(params, x):\n",
    "    return G(x)+D(x)*neural_net_predict(params, x)\n",
    "\n",
    "\n",
    "du=grad(u,1)\n",
    "du2=grad(du,1)\n",
    "\n",
    "def resid(params, x): \n",
    "    return np.square(du2(params,x)+u(params,x)-forcing_fn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vassili/anaconda3/envs/thesis/lib/python3.7/site-packages/autograd/numpy/numpy_vjps.py:444: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return lambda g: g[idxs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start basinhopping.Current func val:[621931.19177112]\n"
     ]
    }
   ],
   "source": [
    "def resid_batch(params, x_batch):\n",
    "    resid_temp=list(map(lambda x: resid(params, x)**2, x_batch))\n",
    "    resid_sum=reduce(lambda x,y:x+y, resid_temp)\n",
    "    return resid_sum/len(x_batch)  \n",
    "def loss_function(params):\n",
    "    return resid_batch(params, x_space)\n",
    "\n",
    "loss_grad=grad(loss_function)\n",
    "\n",
    "x0=init_random_params(1, layer_sizes)\n",
    "optimized_params, fun=modded_basinhopping(loss_function, loss_grad,x0, num_iter=0, max_bfgs_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "plt.clf()\n",
    "ax=plt.gca()\n",
    "\n",
    "sln = np.zeros(nx)\n",
    "\n",
    "for i,x in enumerate(x_space):\n",
    "    sln[i]=u(optimized_params, x)\n",
    "\n",
    "plt.plot(x_space, sln, label='net', marker='2')\n",
    "plt.title(\"Position\")\n",
    "ax.legend()\n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "pde_err=np.zeros(x_space.shape)\n",
    "for i, x in enumerate(x_space):\n",
    "    pde_err[i]=resid(optimized_params, x)\n",
    "\n",
    "plt.plot(x_space, pde_err, label='error')\n",
    "plt.title('PDE residual')\n",
    "\n",
    "pos=np.zeros(x_space.shape)\n",
    "vel=np.zeros(x_space.shape)\n",
    "\n",
    "plt.figure()\n",
    "for i, x in enumerate(x_space):\n",
    "    pos[i]=u(optimized_params, x)\n",
    "    vel[i]=du(optimized_params, x)\n",
    "plt.plot(pos, vel, label='phase plot')\n",
    "plt.title('phase portrait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
